{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Let's built a Vision Transformer (ViT) using RESNet-152","metadata":{"id":"BKsQa17z02GG"}},{"cell_type":"markdown","source":"here we will extract features from images using Resnet 152 and create patches out of them, position embed them, and finally pass it to encoder","metadata":{"id":"hjSFcY9Zd8XP"}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nimport seaborn as sns\n\nfrom zipfile import ZipFile\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom pathlib import Path\nimport time\n\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\n\n!pip install funcyou -q\nfrom funcyou.utils import DotDict, dir_walk\nfrom funcyou.dataset import download_kaggle_resource\nfrom funcyou.preprocessing.image import Patcher\nfrom funcyou.pytorch.utils import calculate_class_weights_from_directory\nfrom funcyou.sklearn.metrics import calculate_results","metadata":{"id":"Qxt4echI0dR1","execution":{"iopub.status.busy":"2023-09-08T13:23:57.363187Z","iopub.execute_input":"2023-09-08T13:23:57.363544Z","iopub.status.idle":"2023-09-08T13:24:14.746860Z","shell.execute_reply.started":"2023-09-08T13:23:57.363503Z","shell.execute_reply":"2023-09-08T13:24:14.745586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device= 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel_name = 'vit'\n\nif model_name=='vitres':\n    model_path = Path('models/vitres.pth')\n\nelif model_name=='vit':\n    model_path = Path('models/vit1.pth')\nelif model_name=='res':\n    model_path = Path('models/res.pth')","metadata":{"id":"hZLczTYRWiNa","execution":{"iopub.status.busy":"2023-09-08T13:24:14.750323Z","iopub.execute_input":"2023-09-08T13:24:14.751350Z","iopub.status.idle":"2023-09-08T13:24:14.778036Z","shell.execute_reply.started":"2023-09-08T13:24:14.751312Z","shell.execute_reply":"2023-09-08T13:24:14.777130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls models","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:14.779232Z","iopub.execute_input":"2023-09-08T13:24:14.782227Z","iopub.status.idle":"2023-09-08T13:24:14.793279Z","shell.execute_reply.started":"2023-09-08T13:24:14.782192Z","shell.execute_reply":"2023-09-08T13:24:14.792344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:14.796284Z","iopub.execute_input":"2023-09-08T13:24:14.797002Z","iopub.status.idle":"2023-09-08T13:24:14.805527Z","shell.execute_reply.started":"2023-09-08T13:24:14.796970Z","shell.execute_reply":"2023-09-08T13:24:14.804557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list(model_path.parent.iterdir())","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:14.807075Z","iopub.execute_input":"2023-09-08T13:24:14.807725Z","iopub.status.idle":"2023-09-08T13:24:14.814395Z","shell.execute_reply.started":"2023-09-08T13:24:14.807690Z","shell.execute_reply":"2023-09-08T13:24:14.813497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path.parent.mkdir(exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:14.816102Z","iopub.execute_input":"2023-09-08T13:24:14.816762Z","iopub.status.idle":"2023-09-08T13:24:14.823473Z","shell.execute_reply.started":"2023-09-08T13:24:14.816729Z","shell.execute_reply":"2023-09-08T13:24:14.822592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"# Create a DotDict instance and initialize it with your configuration\nconfig = DotDict()\nconfig.num_layers = 4\nconfig.resnet_layers = 2\n\nconfig.hidden_dim = 120  # should be mutiple of num_heads\nconfig.mlp_dim = 2048\nconfig.num_heads = 12\nconfig.dropout_rate = 0.1\nconfig.image_size = 512  # should be mutiple of patch_size\nconfig.patch_size = 32   # should be mutiple of 8\nconfig.num_patches = int(config.image_size**2 / config.patch_size**2)\nconfig.num_channels = 3\nconfig.patching_elements = (config.num_channels*config.image_size**2 )//config.num_patches\nconfig.final_resnet_output_dim = 2048\nconfig.num_classes = 2\nconfig.batch_size = 8\nconfig.device = device","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:14.824989Z","iopub.execute_input":"2023-09-08T13:24:14.825724Z","iopub.status.idle":"2023-09-08T13:24:14.834482Z","shell.execute_reply.started":"2023-09-08T13:24:14.825691Z","shell.execute_reply":"2023-09-08T13:24:14.833570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config.num_heads*10","metadata":{"id":"u8rt-55jVDqk","execution":{"iopub.status.busy":"2023-09-08T13:24:14.837804Z","iopub.execute_input":"2023-09-08T13:24:14.838619Z","iopub.status.idle":"2023-09-08T13:24:14.845810Z","shell.execute_reply.started":"2023-09-08T13:24:14.838593Z","shell.execute_reply":"2023-09-08T13:24:14.844805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download Dataset","metadata":{"id":"cGeXqMnDoqdJ"}},{"cell_type":"code","source":"data_dir = Path('../input/pneumonia-chest-x-ray-dataset')","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:14.847056Z","iopub.execute_input":"2023-09-08T13:24:14.848898Z","iopub.status.idle":"2023-09-08T13:24:14.855548Z","shell.execute_reply.started":"2023-09-08T13:24:14.848866Z","shell.execute_reply":"2023-09-08T13:24:14.854484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(data_dir.iterdir())","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:14.860367Z","iopub.execute_input":"2023-09-08T13:24:14.860718Z","iopub.status.idle":"2023-09-08T13:24:14.873190Z","shell.execute_reply.started":"2023-09-08T13:24:14.860688Z","shell.execute_reply":"2023-09-08T13:24:14.872353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = data_dir/'train'\ntest_dir = data_dir/'test'\nval_dir = data_dir/'val'","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:14.874260Z","iopub.execute_input":"2023-09-08T13:24:14.875158Z","iopub.status.idle":"2023-09-08T13:24:14.879644Z","shell.execute_reply.started":"2023-09-08T13:24:14.875116Z","shell.execute_reply":"2023-09-08T13:24:14.878501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_walk(train_dir)","metadata":{"id":"AiVr65gXMwwt","outputId":"6108491e-c6a0-420d-e653-9f17042ce12e","execution":{"iopub.status.busy":"2023-09-08T13:24:14.880952Z","iopub.execute_input":"2023-09-08T13:24:14.881892Z","iopub.status.idle":"2023-09-08T13:24:20.981730Z","shell.execute_reply.started":"2023-09-08T13:24:14.881860Z","shell.execute_reply":"2023-09-08T13:24:20.980750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> There is too much class imbalance pn(can't spell it) : 3875 , normal : 1341, I will take care of it while making dataloader .","metadata":{}},{"cell_type":"code","source":"dir_walk(test_dir)","metadata":{"id":"fNXZ2YKJNLLs","outputId":"1ef0664a-3772-4086-8706-d35041cbbe93","execution":{"iopub.status.busy":"2023-09-08T13:24:20.983328Z","iopub.execute_input":"2023-09-08T13:24:20.983650Z","iopub.status.idle":"2023-09-08T13:24:21.194799Z","shell.execute_reply.started":"2023-09-08T13:24:20.983618Z","shell.execute_reply":"2023-09-08T13:24:21.193691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_walk(val_dir)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:21.196495Z","iopub.execute_input":"2023-09-08T13:24:21.196872Z","iopub.status.idle":"2023-09-08T13:24:21.224521Z","shell.execute_reply.started":"2023-09-08T13:24:21.196836Z","shell.execute_reply":"2023-09-08T13:24:21.223597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define data augmentation transformations for X-ray images\ntrain_transform = transforms.Compose([\n    transforms.RandomRotation(degrees=(-20, 20)),  # Random rotation between -10 and 10 degrees\n    transforms.RandomHorizontalFlip(),            # Random horizontal flip\n    transforms.RandomVerticalFlip(),            # Random vertical flip\n    transforms.RandomResizedCrop(config.image_size, scale=(0.7, 1.3)),  # Randomly resize and crop to 224x224 pixels\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Adjust brightness, contrast, saturation, and hue\n    transforms.ToTensor(),  # Convert to tensor\n   ])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:21.226051Z","iopub.execute_input":"2023-09-08T13:24:21.226364Z","iopub.status.idle":"2023-09-08T13:24:21.233864Z","shell.execute_reply.started":"2023-09-08T13:24:21.226333Z","shell.execute_reply":"2023-09-08T13:24:21.232722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an ImageFolder dataset\ntrain_dataset = ImageFolder(root=train_dir, transform=train_transform)\n\n# since we have imbalanced classes we will create a custom sampler which will sample with given weights\nclass_dist, class_weights, bincount = calculate_class_weights_from_directory(train_dir)\ntrain_sampler = torch.utils.data.WeightedRandomSampler(class_weights, len(class_weights), replacement=True)\nprint('Train')\nprint('given weights:' ,np.unique(class_weights))\nprint('counts: ', Counter(class_weights))\nprint('Train bincount: ', bincount)\n# Create a DataLoader to load the data in batches\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, sampler = train_sampler, num_workers=2)\n","metadata":{"id":"Ydc_VouPYtNW","execution":{"iopub.status.busy":"2023-09-08T13:24:21.235633Z","iopub.execute_input":"2023-09-08T13:24:21.236041Z","iopub.status.idle":"2023-09-08T13:24:21.779734Z","shell.execute_reply.started":"2023-09-08T13:24:21.236007Z","shell.execute_reply":"2023-09-08T13:24:21.778712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as you can see that higher weights are given to elements with low bin count.","metadata":{"id":"DD8mUe8PYtKQ"}},{"cell_type":"code","source":"test_transform = transforms.Compose([\n    transforms.Resize((config.image_size,config.image_size)),\n    transforms.ToTensor()\n   ])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:21.781124Z","iopub.execute_input":"2023-09-08T13:24:21.781746Z","iopub.status.idle":"2023-09-08T13:24:21.787157Z","shell.execute_reply.started":"2023-09-08T13:24:21.781711Z","shell.execute_reply":"2023-09-08T13:24:21.786048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test data , we don't have worry about imbalance in the test data.\ntest_dataset = ImageFolder(root=test_dir, transform=test_transform)\ntest_dataloader  = DataLoader(test_dataset,  batch_size=config.batch_size, shuffle=False, num_workers=2)\n\n# val\nval_dataset = ImageFolder(root=val_dir, transform=test_transform)\nval_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:21.788596Z","iopub.execute_input":"2023-09-08T13:24:21.789233Z","iopub.status.idle":"2023-09-08T13:24:21.804356Z","shell.execute_reply.started":"2023-09-08T13:24:21.789198Z","shell.execute_reply":"2023-09-08T13:24:21.803384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# since we have imbalanced classes we will create a custom sampler which will sample with given weights\nclass_dist, class_weights, bincount = calculate_class_weights_from_directory(test_dir)\ntest_sampler = torch.utils.data.WeightedRandomSampler(class_weights, len(class_weights), replacement=True)\nprint('Test')\nprint('given weights:' ,np.unique(class_weights))\nprint('counts: ', Counter(class_weights))\nprint('Test bincount: ', bincount)\n# Create a DataLoader to load the data in batches\n\nbalanced_test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, sampler = test_sampler, num_workers=2)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:21.806600Z","iopub.execute_input":"2023-09-08T13:24:21.807286Z","iopub.status.idle":"2023-09-08T13:24:21.820565Z","shell.execute_reply.started":"2023-09-08T13:24:21.807244Z","shell.execute_reply":"2023-09-08T13:24:21.819346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterate through the data loader and print the shape of each batch\na = 1\nfor i, j in val_dataloader:\n    print(i.shape)\n    print(j.shape)\n    a += 1\n    if a == 3:\n        break\n","metadata":{"id":"9ZV11k_qqQKI","execution":{"iopub.status.busy":"2023-09-08T13:24:21.822285Z","iopub.execute_input":"2023-09-08T13:24:21.823189Z","iopub.status.idle":"2023-09-08T13:24:22.456613Z","shell.execute_reply.started":"2023-09-08T13:24:21.823154Z","shell.execute_reply":"2023-09-08T13:24:22.455429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class and idx transform","metadata":{"id":"mtzXxiKqT7pA"}},{"cell_type":"code","source":"train_dataset.classes","metadata":{"id":"wn34MI9mxIDx","outputId":"fdb73c3d-38fd-4045-f79c-7fb300fcbfc3","execution":{"iopub.status.busy":"2023-09-08T13:24:22.458815Z","iopub.execute_input":"2023-09-08T13:24:22.459191Z","iopub.status.idle":"2023-09-08T13:24:22.469580Z","shell.execute_reply.started":"2023-09-08T13:24:22.459140Z","shell.execute_reply":"2023-09-08T13:24:22.468441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx2class = {i:name for i, name in enumerate(train_dataset.classes)}\nclass2idx = {name:i for i, name in enumerate(train_dataset.classes)}","metadata":{"id":"9uW7Q04MyQXA","execution":{"iopub.status.busy":"2023-09-08T13:24:22.471415Z","iopub.execute_input":"2023-09-08T13:24:22.471863Z","iopub.status.idle":"2023-09-08T13:24:22.479066Z","shell.execute_reply.started":"2023-09-08T13:24:22.471829Z","shell.execute_reply":"2023-09-08T13:24:22.478146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vision Transformer(VIT)","metadata":{"id":"_c0chr7brV7g"}},{"cell_type":"markdown","source":"![image](https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png)","metadata":{"id":"R6iGJxmL06tF"}},{"cell_type":"code","source":"\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_dim, config.mlp_dim)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.out = nn.Linear(config.mlp_dim, config.hidden_dim)\n\n    def forward(self, x):\n        x = F.gelu(self.dense(x))\n        return self.out(self.dropout(x))\n\nclass Encoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.attention = nn.MultiheadAttention(config.hidden_dim, config.num_heads)\n        self.mlp = MLP(config)\n        self.norm1 = nn.LayerNorm(config.hidden_dim)\n        self.norm2 = nn.LayerNorm(config.hidden_dim)\n        self.attention_weights = None\n\n    def forward(self, x):\n        n_x = self.norm1(x)\n        attn_output, self.attention_weights = self.attention(n_x, n_x, n_x)\n        x = x + attn_output\n        return x + self.mlp(self.norm2(x))\n\nclass EncoderStack(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.layers = nn.ModuleList([Encoder(config) for _ in range(config.num_layers)])\n\n    def forward(self, x):\n        attention_weights = []\n        for layer in self.layers:\n            x = layer(x)\n            attention_weights.append(layer.attention_weights)\n        return x, attention_weights\n","metadata":{"id":"K9ROB49WYFdX","execution":{"iopub.status.busy":"2023-09-08T13:24:22.480206Z","iopub.execute_input":"2023-09-08T13:24:22.481945Z","iopub.status.idle":"2023-09-08T13:24:22.494323Z","shell.execute_reply.started":"2023-09-08T13:24:22.481910Z","shell.execute_reply":"2023-09-08T13:24:22.493357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass VisionTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.positional_embedding_layer = nn.Embedding(config.num_patches, config.hidden_dim).to(config.device)\n        self.positional_embedding_layer.weight.data.uniform_(0, 1)\n\n        self.encoder = EncoderStack(config)  # Create multiple Encoder layers\n        self.dense = nn.Linear(config.hidden_dim, config.hidden_dim)\n        self.final_layer = nn.Linear(config.hidden_dim, 1)\n        self.device = config.device\n        self.patch_linear_proj = nn.Linear(config.patching_elements, config.hidden_dim, bias=False).to(config.device)\n        self.patcher = Patcher((config.patch_size,config.patch_size))\n        # Initialize the class token as a learnable parameter\n#         self.class_token = nn.Parameter(torch.zeros(1, 1, config.hidden_dim))  # Learnable class token\n        self.positional_embeddings = self.build_positional_embedding(config.num_patches, config.hidden_dim)\n        self.sigmoid = nn.Sigmoid()\n\n    def create_patches(self, x):\n        batch = x.shape[0]\n        patches = self.patcher(x).to(self.device)\n#         print('patcher: ', patches.shape,patches.device)\n        patches = self.patch_linear_proj(patches)\n#         print('proj: ', patches.shape,patches.device)\n        # Add a learnable class token to the patch embeddings\n#         class_token = self.class_token.expand(batch, -1, -1)\n#         patches = torch.cat((class_token.to(self.device), patches), dim=1)\n\n        return patches\n\n    def build_positional_embedding(self, num_patches, embed_dim):\n        positions = torch.arange(0, num_patches).view(1, -1).to(self.device)  # Move to the correct device\n#         print('posss: ',positions.device)\n        positional_embeddings = self.positional_embedding_layer(positions)\n        return positional_embeddings\n\n\n    def test(self, x):\n        batch_size = x.shape[0]\n        with torch.no_grad():  # Use torch.no_grad() for inference\n            print('input: ', x.shape, x.device)\n            a = self.create_patches(x)\n            print('patches: ', a.shape, a.device)\n            a = a + self.positional_embeddings.repeat(batch_size, 1, 1)\n            print('pos emb: ', a.shape, a.device)\n            a, b = self.encoder(a)\n            print('encoder: ', a.shape, a.device)\n            a = self.dense(a)\n            print('dense: ', a.shape, a.device)\n            a = self.final_layer(a)\n            print('final: ', a.shape, a.device)\n            a,_ = a.max(dim=1)\n            print('max: ', a.shape, a.device)\n            print('attention weights: ', len(b), '*', b[0].shape)\n            return a, b\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        patch_embeddings = self.create_patches(x)\n        positional_embeddings = self.positional_embeddings.repeat(batch_size, 1, 1)  # Repeat for each batch\n        patch_embeddings = patch_embeddings + positional_embeddings\n        encoded_output, attention_weights_list = self.encoder(patch_embeddings)\n        # Calculate the mean over the 'num_patches' dimension\n        encoded_output = encoded_output.mean(dim=1)\n        # Apply the final linear layer and sigmoid\n        encoded_output = self.final_layer(self.dense(encoded_output))\n        encoded_output = self.sigmoid(encoded_output)\n\n        # Reshape to (batch, 1) shape\n        encoded_output = encoded_output.view(batch_size, 1)\n\n        return encoded_output, attention_weights_list\n\n\nclass Patcher(nn.Module):\n    def __init__(self, patch_size):\n        super(Patcher, self).__init__()\n        self.patch_size = patch_size\n\n    def forward(self, images):\n        if images.dim() == 3:\n            images = images.unsqueeze(0)  # Convert a single image to a batch\n\n        batch_size, channels, height, width = images.size()\n        patch_height, patch_width = self.patch_size\n\n        # Calculate the number of patches in the height and width dimensions\n        num_patches_height = height // patch_height\n        num_patches_width = width // patch_width\n        num_patches = num_patches_height * num_patches_width\n\n        patches = images.unfold(2, patch_height, patch_height).unfold(3, patch_width, patch_width)\n        patches = patches.contiguous().view(batch_size, channels, -1, patch_height, patch_width)\n        patches = patches.permute(0, 2, 3, 4, 1).contiguous().view(batch_size, num_patches, -1)\n\n        return patches","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:22.496066Z","iopub.execute_input":"2023-09-08T13:24:22.496446Z","iopub.status.idle":"2023-09-08T13:24:22.518514Z","shell.execute_reply.started":"2023-09-08T13:24:22.496414Z","shell.execute_reply":"2023-09-08T13:24:22.517503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vit = VisionTransformer(config)\nvit = vit.to(device)\na = torch.rand((1,3,512,512))\noutput, attention_weights = vit.test(a.to(device))","metadata":{"id":"1GFC6nAx5Rr5","outputId":"09dd08ee-4c2d-44e9-a389-db3767382ef3","execution":{"iopub.status.busy":"2023-09-08T13:24:22.519601Z","iopub.execute_input":"2023-09-08T13:24:22.520135Z","iopub.status.idle":"2023-09-08T13:24:26.730026Z","shell.execute_reply.started":"2023-09-08T13:24:22.520102Z","shell.execute_reply":"2023-09-08T13:24:26.729007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output, attention_weights = vit(a)\nprint(output.shape, len(attention_weights))\nprint('outputs: ',output)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:24:26.731566Z","iopub.execute_input":"2023-09-08T13:24:26.731925Z","iopub.status.idle":"2023-09-08T13:24:26.771494Z","shell.execute_reply.started":"2023-09-08T13:24:26.731891Z","shell.execute_reply":"2023-09-08T13:24:26.770523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.BCELoss()\noptimizer = torch.optim.Adam(vit.parameters(), lr = 5e-4)\n\nconfig.device = device\ndevice","metadata":{"id":"o21a7F-OsDi1","outputId":"2d821527-10cf-437d-8f9a-90ed68ea8045","execution":{"iopub.status.busy":"2023-09-08T13:24:26.772955Z","iopub.execute_input":"2023-09-08T13:24:26.773316Z","iopub.status.idle":"2023-09-08T13:24:26.780869Z","shell.execute_reply.started":"2023-09-08T13:24:26.773282Z","shell.execute_reply":"2023-09-08T13:24:26.779852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    vit.load_state_dict(torch.load(model_path))\n    print('weights loaded!')\nexcept:\n    print('No saved model')\n    pass","metadata":{"id":"ccmf6O_Dzjpo","outputId":"ee2b5eea-0a90-48a9-c1b3-72d28d9b63db","execution":{"iopub.status.busy":"2023-09-08T13:24:26.788598Z","iopub.execute_input":"2023-09-08T13:24:26.788892Z","iopub.status.idle":"2023-09-08T13:24:26.815062Z","shell.execute_reply.started":"2023-09-08T13:24:26.788869Z","shell.execute_reply":"2023-09-08T13:24:26.814203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(\n    model,\n    dataloader,\n    optimizer,\n    loss_function,\n    num_epochs=10,\n    device=\"cpu\",\n    data_percent=1.0,\n    steps_per_epoch=None,\n    save_on_every_n_epochs=5,\n    model_path=None,\n):\n    model.to(device)\n    print(f\"{model.__class__.__name__} Running on: {device}\")\n\n    data_size = int(data_percent * len(dataloader)) if steps_per_epoch is None else steps_per_epoch\n\n    # if steps_per_epoch is None:\n    #     steps_per_epoch = len(dataloader) // num_epochs\n\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        total_correct_predictions = 0\n        total_samples = 0\n\n        epoch_progress = tqdm(\n            dataloader, desc=f\"Epoch [{epoch + 1:2}/{num_epochs:2}]\"\n        )\n\n        last_update_time = time.time() - 1.0  # Initialize to ensure the first update\n\n        for j, batch in enumerate(epoch_progress):\n            image, label = batch\n            image = image.to(device)\n            label = label.to(device)\n\n            optimizer.zero_grad()\n\n            outputs, _ = model(image)\n            outputs = outputs.squeeze()\n            predictions = torch.round(outputs)\n#             print('pred: ', predictions.shape)\n#             print('label: ',label.shape, label.dtype)\n# #             print('outsq: ', outputs.shape, outputs.dtype)\n#             print('label: ',label)\n#             print('outsq: ', predictions, predictions.dtype)\n\n            loss = loss_function(outputs, label.to(outputs.dtype))\n            loss.backward(retain_graph=True)\n            optimizer.step()\n\n            total_loss += loss.item()\n            total_correct_predictions += (predictions.to(torch.int32) == label.to(torch.int32)).sum().item()\n            total_samples += label.size()[0]\n\n            formatted_loss = f\"{loss.item():.8f}\"\n            accuracy = (total_correct_predictions / total_samples) * 100\n            formatted_accuracy = f\"{accuracy:.2f}%\"\n            \n            \n            current_time = time.time()\n            if current_time - last_update_time > epoch_progress.mininterval:\n                epoch_progress.set_postfix(\n                    {\"Loss\": formatted_loss, \"Accuracy\": formatted_accuracy}\n                )\n                last_update_time = current_time\n\n            if steps_per_epoch is not None and j + 1 >= steps_per_epoch:\n                break\n        \n        average_loss = total_loss / data_size\n        average_accuracy = (total_correct_predictions / (total_samples + 1e-7)) * 100\n\n        print(\n            f\"\\nEpoch [{epoch + 1:2}/{num_epochs:2}] - Average Loss: {average_loss:.8f} - Average Accuracy: {average_accuracy:.2f}%\"\n        )\n        print()\n\n        if (epoch+1) % save_on_every_n_epochs == 0 and model_path is not None:\n            torch.save(model.state_dict(), model_path)\n","metadata":{"id":"dWfodsp6sDgZ","execution":{"iopub.status.busy":"2023-09-08T13:30:28.554357Z","iopub.execute_input":"2023-09-08T13:30:28.554737Z","iopub.status.idle":"2023-09-08T13:30:28.569864Z","shell.execute_reply.started":"2023-09-08T13:30:28.554701Z","shell.execute_reply":"2023-09-08T13:30:28.568815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(vit, train_dataloader, optimizer, criterion, num_epochs=40, device = device, save_on_every_n_epochs=4, model_path=model_path)","metadata":{"id":"b_6j7ChKsDb1","outputId":"afce2e82-4c98-4389-d80e-4c335ab51e71","execution":{"iopub.status.busy":"2023-09-08T13:30:29.786012Z","iopub.execute_input":"2023-09-08T13:30:29.786375Z","iopub.status.idle":"2023-09-08T13:34:09.913431Z","shell.execute_reply.started":"2023-09-08T13:30:29.786343Z","shell.execute_reply":"2023-09-08T13:34:09.912339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(vit.state_dict(), model_path)","metadata":{"id":"qg1c3icEsDVY","outputId":"a595652d-15a5-49d4-ecc2-1658ce23e761","execution":{"iopub.status.busy":"2023-09-08T13:25:03.509629Z","iopub.status.idle":"2023-09-08T13:25:03.510774Z","shell.execute_reply.started":"2023-09-08T13:25:03.510499Z","shell.execute_reply":"2023-09-08T13:25:03.510526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -la models -h\n","metadata":{"id":"b_fCJt5hsDSV","execution":{"iopub.status.busy":"2023-09-08T13:25:03.512259Z","iopub.status.idle":"2023-09-08T13:25:03.512739Z","shell.execute_reply.started":"2023-09-08T13:25:03.512483Z","shell.execute_reply":"2023-09-08T13:25:03.512504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate_model(\n    model,\n    dataloader,\n    loss_function,\n    device=\"cpu\",\n):\n    model.to(device)\n    print(f\"Validating {model.__class__.__name__} on: {device}\")\n\n    total_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n\n    with torch.inference_mode():\n        validation_progress = tqdm(\n            dataloader, desc=\"Validation\"\n        )\n\n        for batch in validation_progress:\n            image, label = batch\n            image = image.to(device)\n            label = label.to(device)\n\n            outputs, _ = model(image)\n            outputs = outputs.squeeze()\n            predictions = torch.round(outputs)\n\n            loss = loss_function(outputs, label.to(outputs.dtype))\n            \n            total_loss += loss.item()\n            correct_predictions += (predictions.to(torch.int32) == label.to(torch.int32)).sum().item()\n            total_samples += label.size()[0]\n\n            formatted_loss = f\"{loss.item():.8f}\"\n            accuracy = (correct_predictions / total_samples) * 100\n            formatted_accuracy = f\"{accuracy:.2f}%\"\n\n            validation_progress.set_postfix(\n                {\"Loss\": formatted_loss, \"Accuracy\": formatted_accuracy}\n            )\n\n    average_loss = total_loss / len(dataloader)\n    accuracy = (correct_predictions / (total_samples + 1e-7)) * 100\n\n    print(f\"Validation - Average Loss: {average_loss:.8f} - Accuracy: {accuracy:.2f}%\")\n    print()\n\n","metadata":{"id":"koc1zlKxsDHP","execution":{"iopub.status.busy":"2023-09-08T13:37:57.966318Z","iopub.execute_input":"2023-09-08T13:37:57.967435Z","iopub.status.idle":"2023-09-08T13:37:57.978669Z","shell.execute_reply.started":"2023-09-08T13:37:57.967383Z","shell.execute_reply":"2023-09-08T13:37:57.977592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validate_model(vit, val_dataloader, criterion, device = config.device)","metadata":{"id":"6M_Of77IsDEl","outputId":"098d9a4f-3a2a-4a91-f9d1-a66118b3505e","execution":{"iopub.status.busy":"2023-09-08T13:37:58.593915Z","iopub.execute_input":"2023-09-08T13:37:58.594295Z","iopub.status.idle":"2023-09-08T13:37:59.093957Z","shell.execute_reply.started":"2023-09-08T13:37:58.594263Z","shell.execute_reply":"2023-09-08T13:37:59.092749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, dataloader, device=\"cpu\"):\n    model.to(device)\n\n    y_true = []\n    y_pred = []\n\n    with torch.inference_mode():  # Disable gradient computation during evaluation\n        for batch in tqdm(dataloader):\n            images, labels = batch\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs, _ = model(images)\n            outputs = outputs.squeeze()\n            predictions = torch.round(outputs)\n\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(predictions.cpu().numpy())\n\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='binary')\n    recall = recall_score(y_true, y_pred, average='binary')\n    f1 = f1_score(y_true, y_pred, average='binary')\n\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n\n    return y_true, y_pred\n","metadata":{"id":"k-9mmZ2tsDAj","execution":{"iopub.status.busy":"2023-09-08T13:37:59.096228Z","iopub.execute_input":"2023-09-08T13:37:59.096620Z","iopub.status.idle":"2023-09-08T13:37:59.107878Z","shell.execute_reply.started":"2023-09-08T13:37:59.096581Z","shell.execute_reply":"2023-09-08T13:37:59.106627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_cm(ytrue, ypred, title=None):\n    # Create a confusion matrix\n    cm = confusion_matrix(ytrue, ypred)\n\n    # Create a figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n    # Plot the confusion matrix in the first subplot\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[0])\n    axes[0].set_xlabel(\"Predicted Labels\")\n    axes[0].set_ylabel(\"True Labels\")\n    axes[0].set_title(\"Confusion Matrix\" if title is None else title)\n\n    # Calculate and display performance metrics in the second subplot\n    accuracy = accuracy_score(ytrue, ypred)\n    precision = precision_score(ytrue, ypred, average='binary')\n    recall = recall_score(ytrue, ypred, average='binary')\n    f1 = f1_score(ytrue, ypred, average='binary')\n\n    # Set the x and y coordinates for each text label\n    x_pos = 0.2  # Adjust these coordinates as needed\n    y_pos = 0.6\n\n    # Print the text on the plot without x-label and y-label\n    axes[1].text(x_pos, y_pos, f\"Accuracy: {accuracy:.4f}\", fontsize=12)\n    axes[1].text(x_pos, y_pos - 0.1, f\"Precision: {precision:.4f}\", fontsize=12)\n    axes[1].text(x_pos, y_pos - 0.2, f\"Recall: {recall:.4f}\", fontsize=12)\n    axes[1].text(x_pos, y_pos - 0.3, f\"F1 Score: {f1:.4f}\", fontsize=12)\n\n    # Remove x-label and y-label from the second subplot\n    axes[1].axes.get_xaxis().set_visible(False)\n    axes[1].axes.get_yaxis().set_visible(False)\n\n    # Adjust the layout of subplots\n    plt.tight_layout()\n\n    # Show the plot\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:37:59.109630Z","iopub.execute_input":"2023-09-08T13:37:59.110056Z","iopub.status.idle":"2023-09-08T13:37:59.121533Z","shell.execute_reply.started":"2023-09-08T13:37:59.110015Z","shell.execute_reply":"2023-09-08T13:37:59.120514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Testing on Given(unbalanced) test_data\n","metadata":{}},{"cell_type":"code","source":"#testing on test_data\nytrue, ypred = evaluate_model(vit, test_dataloader, device = config.device)","metadata":{"id":"iEuF-WcesC97","outputId":"ae6f360b-4dc1-4b8f-b6d1-f6fb8c6d2085","execution":{"iopub.status.busy":"2023-09-08T13:37:59.347598Z","iopub.execute_input":"2023-09-08T13:37:59.347950Z","iopub.status.idle":"2023-09-08T13:38:13.731017Z","shell.execute_reply.started":"2023-09-08T13:37:59.347921Z","shell.execute_reply":"2023-09-08T13:38:13.728086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.bincount(ytrue), np.bincount(ypred)","metadata":{"id":"0lA3dsv1J5uX","outputId":"3fefa3e0-4fc5-444c-caa8-d95667b427bd","execution":{"iopub.status.busy":"2023-09-08T13:38:13.734223Z","iopub.execute_input":"2023-09-08T13:38:13.735212Z","iopub.status.idle":"2023-09-08T13:38:13.744954Z","shell.execute_reply.started":"2023-09-08T13:38:13.735172Z","shell.execute_reply":"2023-09-08T13:38:13.743752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_cm(ytrue, ypred,title='test data unbalanced')","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:38:13.746137Z","iopub.execute_input":"2023-09-08T13:38:13.746439Z","iopub.status.idle":"2023-09-08T13:38:14.121615Z","shell.execute_reply.started":"2023-09-08T13:38:13.746404Z","shell.execute_reply":"2023-09-08T13:38:14.120570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explaination","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:38:14.124361Z","iopub.execute_input":"2023-09-08T13:38:14.124742Z","iopub.status.idle":"2023-09-08T13:38:14.128820Z","shell.execute_reply.started":"2023-09-08T13:38:14.124707Z","shell.execute_reply":"2023-09-08T13:38:14.127768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*  **Accuracy**  \n*  **Precision**    : (higher the number less likely to predict false positive.) which means model is likely to not classify a healthy person as pneumonia patient \n*  **Recall**       : (higher the number less likely to predict false negetive.) which means model is likely to not classify a pneumonia patient as healthy person.\n* **F1**          : (higher the better) this is sweet spot between precision and recall.","metadata":{}},{"cell_type":"markdown","source":"We should aim for higher Recall. because it is better to classify a healthy person as sick compared to classifing a sick person as healthy. \n","metadata":{}},{"cell_type":"markdown","source":"## Testing on balanced test_data","metadata":{}},{"cell_type":"code","source":"#testing on test_data\nytrue, ypred = evaluate_model(vit, balanced_test_dataloader, device = config.device)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:38:14.140496Z","iopub.execute_input":"2023-09-08T13:38:14.140932Z","iopub.status.idle":"2023-09-08T13:38:29.907782Z","shell.execute_reply.started":"2023-09-08T13:38:14.140899Z","shell.execute_reply":"2023-09-08T13:38:29.906635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.bincount(ytrue), np.bincount(ypred)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:38:29.909922Z","iopub.execute_input":"2023-09-08T13:38:29.910707Z","iopub.status.idle":"2023-09-08T13:38:29.919470Z","shell.execute_reply.started":"2023-09-08T13:38:29.910639Z","shell.execute_reply":"2023-09-08T13:38:29.918406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_cm(ytrue, ypred,title='test data balanced')","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:38:29.921145Z","iopub.execute_input":"2023-09-08T13:38:29.921505Z","iopub.status.idle":"2023-09-08T13:38:30.282553Z","shell.execute_reply.started":"2023-09-08T13:38:29.921470Z","shell.execute_reply":"2023-09-08T13:38:30.281620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 's performance can be improved with more unbiased data. ","metadata":{}},{"cell_type":"markdown","source":"In conclusion, the model's performance and capabilities are influenced by various factors, including its architecture, training data, and the problem it was designed to solve. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}