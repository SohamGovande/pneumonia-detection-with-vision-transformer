{"cells":[{"cell_type":"markdown","metadata":{"id":"BKsQa17z02GG"},"source":["# Let's built a Vision Transformer (ViT) using RESNet-152"]},{"cell_type":"markdown","metadata":{"id":"hjSFcY9Zd8XP"},"source":["here we will extract features from images using Resnet 152 and create patches out of them, position embed them, and finally pass it to encoder"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:23:57.363544Z","iopub.status.busy":"2023-09-08T13:23:57.363187Z","iopub.status.idle":"2023-09-08T13:24:14.746860Z","shell.execute_reply":"2023-09-08T13:24:14.745586Z","shell.execute_reply.started":"2023-09-08T13:23:57.363503Z"},"id":"Qxt4echI0dR1","trusted":true},"outputs":[],"source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n","import seaborn as sns\n","\n","from zipfile import ZipFile\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","from collections import Counter\n","from pathlib import Path\n","import time\n","\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","\n","# !pip install funcyou -q\n","from funcyou.utils import DotDict, dir_walk\n","from funcyou.dataset import download_kaggle_resource\n","# from funcyou.preprocessing.image import Patcher\n","# from funcyou.pytorch.utils import calculate_class_weights_from_directory\n","# from funcyou.sklearn.metrics import calculate_results"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:14.751350Z","iopub.status.busy":"2023-09-08T13:24:14.750323Z","iopub.status.idle":"2023-09-08T13:24:14.778036Z","shell.execute_reply":"2023-09-08T13:24:14.777130Z","shell.execute_reply.started":"2023-09-08T13:24:14.751312Z"},"id":"hZLczTYRWiNa","trusted":true},"outputs":[],"source":["device= 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","model_name = 'vit'\n","\n","if model_name=='vitres':\n","    model_path = Path('models/vitres.pth')\n","\n","elif model_name=='vit':\n","    model_path = Path('models/vit1.pth')\n","elif model_name=='res':\n","    model_path = Path('models/res.pth')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:14.782227Z","iopub.status.busy":"2023-09-08T13:24:14.779232Z","iopub.status.idle":"2023-09-08T13:24:14.793279Z","shell.execute_reply":"2023-09-08T13:24:14.792344Z","shell.execute_reply.started":"2023-09-08T13:24:14.782192Z"},"trusted":true},"outputs":[],"source":["# !ls models"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:14.797002Z","iopub.status.busy":"2023-09-08T13:24:14.796284Z","iopub.status.idle":"2023-09-08T13:24:14.805527Z","shell.execute_reply":"2023-09-08T13:24:14.804557Z","shell.execute_reply.started":"2023-09-08T13:24:14.796970Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PosixPath('models/vit1.pth')"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["model_path"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:14.807725Z","iopub.status.busy":"2023-09-08T13:24:14.807075Z","iopub.status.idle":"2023-09-08T13:24:14.814395Z","shell.execute_reply":"2023-09-08T13:24:14.813497Z","shell.execute_reply.started":"2023-09-08T13:24:14.807690Z"},"trusted":true},"outputs":[],"source":["# list(model_path.parent.iterdir())"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:14.816762Z","iopub.status.busy":"2023-09-08T13:24:14.816102Z","iopub.status.idle":"2023-09-08T13:24:14.823473Z","shell.execute_reply":"2023-09-08T13:24:14.822592Z","shell.execute_reply.started":"2023-09-08T13:24:14.816729Z"},"trusted":true},"outputs":[],"source":["model_path.parent.mkdir(exist_ok=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:14.825724Z","iopub.status.busy":"2023-09-08T13:24:14.824989Z","iopub.status.idle":"2023-09-08T13:24:14.834482Z","shell.execute_reply":"2023-09-08T13:24:14.833570Z","shell.execute_reply.started":"2023-09-08T13:24:14.825691Z"},"trusted":true},"outputs":[],"source":["# Create a DotDict instance and initialize it with your configuration\n","config = DotDict()\n","config.num_layers = 4\n","config.resnet_layers = 2\n","\n","config.hidden_dim = 120  # should be mutiple of num_heads\n","config.mlp_dim = 2048\n","config.num_heads = 12\n","config.dropout_rate = 0.1\n","config.image_size = 512  # should be mutiple of patch_size\n","config.patch_size = 32   # should be mutiple of 8\n","config.num_patches = int(config.image_size**2 / config.patch_size**2)\n","config.num_channels = 3\n","config.patching_elements = (config.num_channels*config.image_size**2 )//config.num_patches\n","config.final_resnet_output_dim = 2048\n","config.num_classes = 2\n","config.batch_size = 8\n","config.device = device"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:14.838619Z","iopub.status.busy":"2023-09-08T13:24:14.837804Z","iopub.status.idle":"2023-09-08T13:24:14.845810Z","shell.execute_reply":"2023-09-08T13:24:14.844805Z","shell.execute_reply.started":"2023-09-08T13:24:14.838593Z"},"id":"u8rt-55jVDqk","trusted":true},"outputs":[{"data":{"text/plain":["120"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["config.num_heads*10"]},{"cell_type":"markdown","metadata":{"id":"cGeXqMnDoqdJ"},"source":["# Download Dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:14.848898Z","iopub.status.busy":"2023-09-08T13:24:14.847056Z","iopub.status.idle":"2023-09-08T13:24:14.855548Z","shell.execute_reply":"2023-09-08T13:24:14.854484Z","shell.execute_reply.started":"2023-09-08T13:24:14.848866Z"},"trusted":true},"outputs":[],"source":["data_dir = Path('dataset')"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:14.860718Z","iopub.status.busy":"2023-09-08T13:24:14.860367Z","iopub.status.idle":"2023-09-08T13:24:14.873190Z","shell.execute_reply":"2023-09-08T13:24:14.872353Z","shell.execute_reply.started":"2023-09-08T13:24:14.860688Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[PosixPath('dataset/val'),\n"," PosixPath('dataset/train'),\n"," PosixPath('dataset/test')]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["list(data_dir.iterdir())"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:14.875158Z","iopub.status.busy":"2023-09-08T13:24:14.874260Z","iopub.status.idle":"2023-09-08T13:24:14.879644Z","shell.execute_reply":"2023-09-08T13:24:14.878501Z","shell.execute_reply.started":"2023-09-08T13:24:14.875116Z"},"trusted":true},"outputs":[],"source":["train_dir = data_dir/'train'\n","test_dir = data_dir/'test'\n","val_dir = data_dir/'val'"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:14.881892Z","iopub.status.busy":"2023-09-08T13:24:14.880952Z","iopub.status.idle":"2023-09-08T13:24:20.981730Z","shell.execute_reply":"2023-09-08T13:24:20.980750Z","shell.execute_reply.started":"2023-09-08T13:24:14.881860Z"},"id":"AiVr65gXMwwt","outputId":"6108491e-c6a0-420d-e653-9f17042ce12e","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>directory</th>\n","      <th>base</th>\n","      <th>folders</th>\n","      <th>video</th>\n","      <th>music</th>\n","      <th>photos</th>\n","      <th>application/zip</th>\n","      <th>documents</th>\n","      <th>others</th>\n","      <th>total files</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>dataset/train</td>\n","      <td>train</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>dataset/train/pneumonia</td>\n","      <td>pneumonia</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3850</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3850</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>dataset/train/normal</td>\n","      <td>normal</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1340</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1340</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 directory       base  folders  video  music  photos  \\\n","0            dataset/train      train        2      0      0       0   \n","1  dataset/train/pneumonia  pneumonia        0      0      0    3850   \n","2     dataset/train/normal     normal        0      0      0    1340   \n","\n","   application/zip  documents  others  total files  \n","0                0          0       0            0  \n","1                0          0       0         3850  \n","2                0          0       0         1340  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["dir_walk(train_dir)"]},{"cell_type":"markdown","metadata":{},"source":["> There is too much class imbalance pn(can't spell it) : 3875 , normal : 1341, I will take care of it while making dataloader ."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:20.983650Z","iopub.status.busy":"2023-09-08T13:24:20.983328Z","iopub.status.idle":"2023-09-08T13:24:21.194799Z","shell.execute_reply":"2023-09-08T13:24:21.193691Z","shell.execute_reply.started":"2023-09-08T13:24:20.983618Z"},"id":"fNXZ2YKJNLLs","outputId":"1ef0664a-3772-4086-8706-d35041cbbe93","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>directory</th>\n","      <th>base</th>\n","      <th>folders</th>\n","      <th>video</th>\n","      <th>music</th>\n","      <th>photos</th>\n","      <th>application/zip</th>\n","      <th>documents</th>\n","      <th>others</th>\n","      <th>total files</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>dataset/test</td>\n","      <td>test</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>dataset/test/pneumonia</td>\n","      <td>pneumonia</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>387</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>387</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>dataset/test/normal</td>\n","      <td>normal</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>231</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>231</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                directory       base  folders  video  music  photos  \\\n","0            dataset/test       test        2      0      0       0   \n","1  dataset/test/pneumonia  pneumonia        0      0      0     387   \n","2     dataset/test/normal     normal        0      0      0     231   \n","\n","   application/zip  documents  others  total files  \n","0                0          0       0            0  \n","1                0          0       0          387  \n","2                0          0       0          231  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["dir_walk(test_dir)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:21.196872Z","iopub.status.busy":"2023-09-08T13:24:21.196495Z","iopub.status.idle":"2023-09-08T13:24:21.224521Z","shell.execute_reply":"2023-09-08T13:24:21.223597Z","shell.execute_reply.started":"2023-09-08T13:24:21.196836Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>directory</th>\n","      <th>base</th>\n","      <th>folders</th>\n","      <th>video</th>\n","      <th>music</th>\n","      <th>photos</th>\n","      <th>application/zip</th>\n","      <th>documents</th>\n","      <th>others</th>\n","      <th>total files</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>dataset/val</td>\n","      <td>val</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>dataset/val/pneumonia</td>\n","      <td>pneumonia</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>dataset/val/normal</td>\n","      <td>normal</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               directory       base  folders  video  music  photos  \\\n","0            dataset/val        val        2      0      0       0   \n","1  dataset/val/pneumonia  pneumonia        0      0      0       8   \n","2     dataset/val/normal     normal        0      0      0       8   \n","\n","   application/zip  documents  others  total files  \n","0                0          0       0            0  \n","1                0          0       0            8  \n","2                0          0       0            8  "]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["dir_walk(val_dir)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:21.226364Z","iopub.status.busy":"2023-09-08T13:24:21.226051Z","iopub.status.idle":"2023-09-08T13:24:21.233864Z","shell.execute_reply":"2023-09-08T13:24:21.232722Z","shell.execute_reply.started":"2023-09-08T13:24:21.226333Z"},"trusted":true},"outputs":[],"source":["# Define data augmentation transformations for X-ray images\n","train_transform = transforms.Compose([\n","    transforms.RandomRotation(degrees=(-20, 20)),  # Random rotation between -10 and 10 degrees\n","    transforms.RandomHorizontalFlip(),            # Random horizontal flip\n","    transforms.RandomVerticalFlip(),            # Random vertical flip\n","    transforms.RandomResizedCrop(config.image_size, scale=(0.7, 1.3)),  # Randomly resize and crop to 224x224 pixels\n","    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Adjust brightness, contrast, saturation, and hue\n","    transforms.ToTensor(),  # Convert to tensor\n","   ])\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["from pathlib import Path\n","\n","import numpy as np\n","\n","\n","def calculate_class_weights_from_directory(directory_path):\n","  \"\"\"\n","  This function calculates the class weights for a given directory of images.\n","\n","  Args:\n","    directory_path (str): The path to the directory of images.\n","\n","  Returns:\n","    (list, list, np.ndarray): The class distribution, class weights, and bincount.\n","  \"\"\"\n","\n","  # Create a Path object for the directory\n","  directory = Path(directory_path)\n","\n","  # Get a list of subdirectories (classes)\n","  classes = sorted([d.name for d in directory.iterdir() if d.is_dir()])\n","\n","  # Initialize class_dist to store class labels\n","  class_dist = []\n","\n","  # Iterate through each class directory\n","  for idx, class_name in enumerate(classes):\n","    class_path = directory / class_name\n","    num_files = len(list(class_path.glob('*')))\n","    class_dist.extend([idx] * num_files)\n","\n","  # Calculate the class counts using bincount\n","  bincount = np.bincount(class_dist)\n","\n","  # Calculate class weights as 1 / bincount for each class\n","  class_weights = 1.0 / bincount[class_dist]\n","\n","  return class_dist, class_weights, bincount"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["import random\n","from collections import Counter"]},{"cell_type":"markdown","metadata":{},"source":["# Explaining Weightedsampler"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"data":{"text/plain":["array([1, 2, 2, ..., 4, 4, 3])"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["a = np.array([1,2,2,2,4,4,3,4,4,4,4,3,1,2,2,2,4,4,3,4,4,4,4,3]*50)\n","a"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["\n","def class_weights_from_array(a):\n","    # Calculate the class counts using bincount\n","    bincount = np.bincount(a)\n","\n","    # Calculate class weights as 1 / bincount for each class\n","    class_weights = 1.0 / bincount[a]\n","    counter = Counter(a)\n","    print(counter)\n","    return class_weights\n"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Counter({4: 600, 2: 300, 3: 200, 1: 100})\n"]}],"source":["cw = class_weights_from_array(a)"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["sampler = torch.utils.data.WeightedRandomSampler(cw, len(cw), replacement=True)\n"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["ds = DataLoader(a, sampler=sampler, batch_size=50)\n","ds2 = DataLoader(a, batch_size=50)"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["using sampler\n","tensor([1, 1, 2, 1, 2, 3, 2, 3, 2, 4, 1, 1, 4, 1, 3, 4, 4, 4, 4, 4, 3, 1, 1, 3,\n","        1, 3, 2, 4, 4, 4, 4, 3, 4, 2, 3, 4, 3, 4, 3, 1, 3, 4, 3, 4, 1, 2, 2, 3,\n","        2, 3]) tensor([ 0, 11,  9, 14, 16])\n","tensor([3, 2, 4, 1, 1, 4, 3, 1, 4, 1, 2, 1, 2, 4, 2, 2, 3, 1, 3, 3, 2, 1, 3, 4,\n","        3, 2, 1, 3, 2, 1, 3, 2, 2, 2, 1, 1, 1, 2, 2, 4, 1, 2, 2, 4, 3, 1, 1, 2,\n","        1, 4]) tensor([ 0, 16, 16, 10,  8])\n","tensor([4, 3, 1, 3, 1, 3, 1, 3, 3, 4, 3, 3, 3, 2, 1, 4, 2, 3, 4, 4, 3, 1, 4, 2,\n","        2, 4, 4, 1, 3, 3, 3, 4, 4, 1, 3, 3, 3, 2, 1, 3, 2, 4, 3, 3, 1, 1, 4, 4,\n","        4, 3]) tensor([ 0, 10,  6, 20, 14])\n","tensor([3, 3, 1, 4, 1, 4, 4, 2, 1, 1, 3, 2, 3, 1, 2, 3, 2, 3, 2, 1, 1, 2, 2, 3,\n","        2, 2, 1, 1, 4, 4, 3, 4, 1, 2, 2, 1, 3, 2, 4, 2, 4, 1, 4, 2, 2, 3, 3, 3,\n","        2, 3]) tensor([ 0, 12, 16, 13,  9])\n","tensor([3, 4, 3, 4, 2, 1, 1, 4, 1, 2, 4, 4, 3, 2, 3, 4, 1, 2, 1, 4, 1, 1, 4, 2,\n","        1, 4, 4, 4, 3, 2, 4, 3, 4, 3, 3, 4, 3, 4, 3, 2, 1, 1, 4, 1, 3, 4, 2, 4,\n","        2, 4]) tensor([ 0, 11,  9, 11, 19])\n","tensor([2, 2, 4, 1, 2, 2, 3, 3, 3, 3, 1, 2, 4, 1, 1, 1, 2, 4, 1, 3, 3, 4, 1, 3,\n","        2, 2, 2, 2, 4, 4, 3, 4, 4, 3, 1, 3, 4, 4, 3, 1, 1, 2, 3, 2, 3, 2, 2, 3,\n","        4, 1]) tensor([ 0, 11, 14, 14, 11])\n","tensor([2, 1, 3, 1, 2, 4, 4, 1, 3, 4, 2, 1, 2, 3, 1, 1, 1, 1, 3, 1, 2, 4, 3, 1,\n","        3, 3, 1, 4, 2, 1, 2, 4, 1, 2, 3, 1, 3, 4, 3, 2, 2, 1, 2, 2, 1, 3, 4, 3,\n","        4, 2]) tensor([ 0, 16, 13, 12,  9])\n","tensor([3, 1, 1, 3, 1, 1, 2, 1, 2, 1, 2, 4, 2, 2, 1, 3, 1, 4, 4, 1, 2, 3, 2, 3,\n","        3, 4, 4, 1, 2, 3, 4, 1, 4, 2, 4, 2, 2, 2, 3, 3, 1, 2, 3, 2, 4, 3, 3, 4,\n","        3, 2]) tensor([ 0, 12, 15, 13, 10])\n","tensor([4, 4, 2, 1, 2, 1, 1, 1, 4, 1, 1, 4, 1, 3, 2, 4, 2, 3, 3, 3, 4, 2, 1, 3,\n","        4, 2, 2, 1, 1, 4, 4, 2, 3, 4, 4, 1, 2, 2, 4, 3, 1, 1, 3, 2, 4, 3, 1, 1,\n","        3, 2]) tensor([ 0, 15, 12, 10, 13])\n","tensor([1, 3, 4, 3, 4, 1, 1, 2, 4, 1, 2, 1, 1, 2, 2, 2, 1, 2, 4, 4, 1, 3, 2, 4,\n","        4, 4, 4, 3, 4, 1, 1, 4, 2, 4, 1, 4, 2, 2, 3, 1, 2, 1, 3, 1, 3, 4, 4, 4,\n","        2, 3]) tensor([ 0, 14, 12,  8, 16])\n","tensor([1, 1, 2, 4, 3, 3, 4, 2, 2, 3, 1, 3, 4, 4, 3, 2, 1, 4, 2, 1, 2, 4, 4, 2,\n","        2, 3, 3, 4, 2, 1, 1, 2, 3, 4, 3, 3, 2, 2, 3, 4, 1, 1, 1, 3, 1, 2, 1, 2,\n","        3, 3]) tensor([ 0, 12, 14, 14, 10])\n","tensor([4, 1, 4, 4, 2, 1, 4, 4, 3, 3, 1, 2, 4, 1, 4, 2, 1, 1, 2, 1, 4, 1, 1, 2,\n","        1, 2, 3, 2, 3, 1, 3, 3, 3, 2, 2, 2, 2, 1, 4, 2, 1, 1, 4, 2, 3, 1, 3, 2,\n","        4, 2]) tensor([ 0, 15, 15,  9, 11])\n","tensor([4, 4, 4, 2, 2, 1, 1, 2, 4, 3, 1, 2, 1, 2, 2, 2, 2, 4, 2, 1, 1, 1, 3, 2,\n","        4, 2, 4, 2, 4, 1, 2, 2, 4, 2, 3, 1, 3, 4, 1, 4, 1, 2, 3, 3, 2, 1, 4, 4,\n","        2, 2]) tensor([ 0, 12, 19,  6, 13])\n","tensor([1, 1, 2, 1, 4, 4, 1, 2, 2, 3, 4, 4, 3, 1, 2, 4, 3, 3, 3, 3, 2, 3, 3, 2,\n","        4, 1, 3, 1, 1, 4, 2, 2, 1, 3, 4, 3, 2, 1, 1, 3, 4, 1, 1, 3, 2, 1, 4, 4,\n","        4, 1]) tensor([ 0, 15, 10, 13, 12])\n","tensor([1, 2, 4, 1, 3, 4, 3, 3, 1, 3, 1, 3, 1, 4, 3, 1, 4, 2, 4, 4, 1, 4, 1, 3,\n","        3, 3, 1, 2, 3, 2, 3, 1, 2, 2, 2, 4, 4, 1, 1, 1, 2, 1, 4, 3, 2, 4, 4, 3,\n","        2, 4]) tensor([ 0, 14, 10, 13, 13])\n","tensor([1, 1, 3, 4, 4, 2, 1, 3, 3, 2, 4, 4, 1, 1, 2, 3, 1, 2, 2, 1, 2, 1, 2, 4,\n","        2, 2, 4, 3, 3, 2, 1, 3, 1, 3, 4, 4, 3, 2, 4, 4, 1, 4, 3, 1, 2, 2, 4, 3,\n","        4, 4]) tensor([ 0, 12, 13, 11, 14])\n","tensor([2, 3, 2, 3, 3, 3, 2, 2, 1, 2, 3, 3, 2, 1, 1, 2, 1, 1, 3, 1, 2, 2, 4, 3,\n","        3, 2, 3, 1, 1, 2, 2, 3, 3, 3, 2, 1, 2, 3, 1, 1, 1, 2, 2, 3, 3, 3, 2, 4,\n","        1, 1]) tensor([ 0, 14, 17, 17,  2])\n","tensor([4, 2, 3, 1, 4, 3, 1, 1, 3, 3, 4, 2, 1, 2, 2, 4, 1, 1, 1, 3, 1, 1, 3, 1,\n","        3, 2, 3, 4, 4, 3, 2, 2, 3, 3, 4, 3, 2, 1, 3, 2, 4, 3, 2, 2, 2, 2, 3, 1,\n","        4, 2]) tensor([ 0, 12, 14, 15,  9])\n","tensor([3, 2, 4, 1, 4, 1, 4, 3, 1, 1, 1, 3, 2, 4, 4, 1, 4, 1, 2, 4, 4, 2, 1, 2,\n","        3, 3, 1, 1, 3, 2, 2, 2, 2, 3, 4, 1, 3, 3, 3, 2, 1, 3, 2, 4, 1, 2, 4, 1,\n","        1, 1]) tensor([ 0, 16, 12, 11, 11])\n","tensor([3, 2, 4, 3, 1, 2, 3, 2, 3, 3, 4, 1, 3, 2, 4, 4, 2, 2, 1, 2, 1, 1, 1, 2,\n","        3, 3, 2, 3, 3, 4, 4, 3, 1, 1, 2, 3, 2, 2, 3, 1, 4, 2, 3, 4, 3, 1, 4, 1,\n","        4, 4]) tensor([ 0, 11, 13, 15, 11])\n","tensor([1, 2, 1, 1, 4, 2, 4, 1, 1, 1, 2, 2, 2, 4, 2, 4, 3, 2, 4, 4, 4, 2, 4, 2,\n","        3, 2, 3, 4, 3, 4, 3, 4, 4, 1, 3, 1, 1, 1, 4, 1, 4, 2, 3, 1, 1, 1, 2, 4,\n","        1, 3]) tensor([ 0, 15, 12,  8, 15])\n","tensor([1, 1, 1, 2, 1, 4, 1, 2, 1, 2, 4, 3, 3, 3, 4, 1, 3, 3, 1, 1, 4, 3, 1, 1,\n","        4, 3, 1, 3, 3, 2, 4, 1, 2, 4, 4, 4, 2, 3, 3, 4, 3, 4, 4, 2, 3, 1, 2, 3,\n","        2, 2]) tensor([ 0, 14, 10, 14, 12])\n","tensor([2, 2, 4, 3, 4, 3, 1, 2, 2, 1, 1, 4, 1, 2, 4, 3, 4, 1, 2, 3, 4, 4, 4, 4,\n","        1, 2, 4, 1, 4, 3, 1, 4, 4, 3, 4, 4, 2, 4, 4, 3, 2, 2, 2, 3, 1, 3, 2, 3,\n","        2, 3]) tensor([ 0,  9, 13, 11, 17])\n","tensor([3, 3, 3, 1, 2, 2, 3, 2, 3, 1, 1, 1, 3, 1, 2, 2, 3, 1, 2, 4, 1, 3, 2, 1,\n","        4, 4, 3, 2, 3, 4, 2, 3, 1, 4, 2, 3, 4, 4, 2, 1, 3, 2, 1, 1, 2, 1, 2, 4,\n","        2, 2]) tensor([ 0, 13, 16, 13,  8])\n","sample mean:  tensor([ 0.0000, 13.0000, 12.9167, 12.2917, 11.7917])\n","not using sampler\n","tensor([1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3,\n","        1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3,\n","        1, 2]) tensor([ 0,  5, 13,  8, 24])\n","tensor([2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2,\n","        2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2,\n","        2, 2]) tensor([ 0,  4, 14,  8, 24])\n","tensor([4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2,\n","        4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2,\n","        4, 4]) tensor([ 0,  4, 12,  8, 26])\n","tensor([3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4,\n","        3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4,\n","        3, 4]) tensor([ 0,  4, 12,  9, 25])\n","tensor([4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4,\n","        4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4,\n","        4, 4]) tensor([ 0,  4, 12,  8, 26])\n","tensor([4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4,\n","        4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4,\n","        4, 3]) tensor([ 0,  4, 12,  9, 25])\n","tensor([1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3,\n","        1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3,\n","        1, 2]) tensor([ 0,  5, 13,  8, 24])\n","tensor([2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2,\n","        2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2,\n","        2, 2]) tensor([ 0,  4, 14,  8, 24])\n","tensor([4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2,\n","        4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2,\n","        4, 4]) tensor([ 0,  4, 12,  8, 26])\n","tensor([3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4,\n","        3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4,\n","        3, 4]) tensor([ 0,  4, 12,  9, 25])\n","tensor([4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4,\n","        4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4,\n","        4, 4]) tensor([ 0,  4, 12,  8, 26])\n","tensor([4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4,\n","        4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4,\n","        4, 3]) tensor([ 0,  4, 12,  9, 25])\n","tensor([1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3,\n","        1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3,\n","        1, 2]) tensor([ 0,  5, 13,  8, 24])\n","tensor([2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2,\n","        2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2,\n","        2, 2]) tensor([ 0,  4, 14,  8, 24])\n","tensor([4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2,\n","        4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2,\n","        4, 4]) tensor([ 0,  4, 12,  8, 26])\n","tensor([3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4,\n","        3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4,\n","        3, 4]) tensor([ 0,  4, 12,  9, 25])\n","tensor([4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4,\n","        4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4,\n","        4, 4]) tensor([ 0,  4, 12,  8, 26])\n","tensor([4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4,\n","        4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4,\n","        4, 3]) tensor([ 0,  4, 12,  9, 25])\n","tensor([1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3,\n","        1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3,\n","        1, 2]) tensor([ 0,  5, 13,  8, 24])\n","tensor([2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2,\n","        2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2,\n","        2, 2]) tensor([ 0,  4, 14,  8, 24])\n","tensor([4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2,\n","        4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2,\n","        4, 4]) tensor([ 0,  4, 12,  8, 26])\n","tensor([3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4,\n","        3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4,\n","        3, 4]) tensor([ 0,  4, 12,  9, 25])\n","tensor([4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4,\n","        4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4,\n","        4, 4]) tensor([ 0,  4, 12,  8, 26])\n","tensor([4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4,\n","        4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4, 4, 3, 1, 2, 2, 2, 4, 4, 3, 4, 4, 4,\n","        4, 3]) tensor([ 0,  4, 12,  9, 25])\n","sample mean:  tensor([ 0.0000,  4.1667, 12.5000,  8.3333, 25.0000])\n"]}],"source":["\n","samples  = 1000\n","print('using sampler')\n","bins = []\n","for i, j in enumerate(ds):\n","    counts = torch.bincount(j)\n","    print(j, counts )\n","    bins.append(counts)\n","    if i == samples:\n","        break\n","sample_mean = torch.mean(torch.stack(bins).to(torch.float32), dim=0)\n","print('sample mean: ', sample_mean)\n","\n","\n","print('not using sampler')\n","bins = []\n","for i, j in enumerate(ds2):\n","    counts = torch.bincount(j)\n","    print(j, counts )\n","    bins.append(counts)\n","    if i == samples:\n","        break\n","sample_mean = torch.mean(torch.stack(bins).to(torch.float32), dim=0)\n","print('sample mean: ', sample_mean)"]},{"cell_type":"markdown","metadata":{},"source":["giving class weights helps classes with low probability , a higher chance of getting picked up. here distribution of classes is not normal. but when we use weighted sampler all class have similar probabilities.\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:21.236041Z","iopub.status.busy":"2023-09-08T13:24:21.235633Z","iopub.status.idle":"2023-09-08T13:24:21.779734Z","shell.execute_reply":"2023-09-08T13:24:21.778712Z","shell.execute_reply.started":"2023-09-08T13:24:21.236007Z"},"id":"Ydc_VouPYtNW","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train\n","given weights: [0.00025974 0.00074627]\n","counts:  Counter({0.00025974025974025974: 3850, 0.0007462686567164179: 1340})\n","Train bincount:  [1340 3850]\n"]}],"source":["# Create an ImageFolder dataset\n","train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n","\n","# since we have imbalanced classes we will create a custom sampler which will sample with given weights\n","class_dist, class_weights, bincount = calculate_class_weights_from_directory(train_dir)\n","train_sampler = torch.utils.data.WeightedRandomSampler(class_weights, len(class_weights), replacement=True)\n","print('Train')\n","print('given weights:' ,np.unique(class_weights))\n","print('counts: ', Counter(class_weights))\n","print('Train bincount: ', bincount)\n","# Create a DataLoader to load the data in batches\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, sampler = train_sampler, num_workers=2)\n"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["cc = [i[1] for i in train_dataset.imgs]"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"data":{"text/plain":["Counter({1: 3850, 0: 1340})"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["Counter(cc)"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["bins = [j for i, j in train_dataloader]"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [8] at entry 0 and [6] at entry 648","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/t/aproject/x-ray/vit-pnemonia-detection-kaggle.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/t/aproject/x-ray/vit-pnemonia-detection-kaggle.ipynb#Y135sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sample_mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39;49mstack(bins)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/t/aproject/x-ray/vit-pnemonia-detection-kaggle.ipynb#Y135sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msample mean: \u001b[39m\u001b[39m'\u001b[39m, sample_mean)\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [8] at entry 0 and [6] at entry 648"]}],"source":["sample_mean = torch.mean(torch.stack(bins).to(torch.float32), dim=0)\n","print('sample mean: ', sample_mean)"]},{"cell_type":"markdown","metadata":{"id":"DD8mUe8PYtKQ"},"source":["as you can see that higher weights are given to elements with low bin count."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:21.781746Z","iopub.status.busy":"2023-09-08T13:24:21.781124Z","iopub.status.idle":"2023-09-08T13:24:21.787157Z","shell.execute_reply":"2023-09-08T13:24:21.786048Z","shell.execute_reply.started":"2023-09-08T13:24:21.781711Z"},"trusted":true},"outputs":[],"source":["test_transform = transforms.Compose([\n","    transforms.Resize((config.image_size,config.image_size)),\n","    transforms.ToTensor()\n","   ])\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:21.789233Z","iopub.status.busy":"2023-09-08T13:24:21.788596Z","iopub.status.idle":"2023-09-08T13:24:21.804356Z","shell.execute_reply":"2023-09-08T13:24:21.803384Z","shell.execute_reply.started":"2023-09-08T13:24:21.789198Z"},"trusted":true},"outputs":[],"source":["# test data , we don't have worry about imbalance in the test data.\n","test_dataset = ImageFolder(root=test_dir, transform=test_transform)\n","test_dataloader  = DataLoader(test_dataset,  batch_size=config.batch_size, shuffle=False, num_workers=2)\n","\n","# val\n","val_dataset = ImageFolder(root=val_dir, transform=test_transform)\n","val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/plain":["['dataset/val/normal/NORMAL2-IM-1427-0001.jpeg',\n"," 'dataset/val/normal/NORMAL2-IM-1430-0001.jpeg',\n"," 'dataset/val/normal/NORMAL2-IM-1431-0001.jpeg',\n"," 'dataset/val/normal/NORMAL2-IM-1436-0001.jpeg',\n"," 'dataset/val/normal/NORMAL2-IM-1437-0001.jpeg',\n"," 'dataset/val/normal/NORMAL2-IM-1438-0001.jpeg',\n"," 'dataset/val/normal/NORMAL2-IM-1440-0001.jpeg',\n"," 'dataset/val/normal/NORMAL2-IM-1442-0001.jpeg',\n"," 'dataset/val/pneumonia/person1946_bacteria_4874.jpeg',\n"," 'dataset/val/pneumonia/person1946_bacteria_4875.jpeg',\n"," 'dataset/val/pneumonia/person1947_bacteria_4876.jpeg',\n"," 'dataset/val/pneumonia/person1949_bacteria_4880.jpeg',\n"," 'dataset/val/pneumonia/person1950_bacteria_4881.jpeg',\n"," 'dataset/val/pneumonia/person1951_bacteria_4882.jpeg',\n"," 'dataset/val/pneumonia/person1952_bacteria_4883.jpeg',\n"," 'dataset/val/pneumonia/person1954_bacteria_4886.jpeg']"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["image_paths = [i[0] for i in  val_dataset.imgs]\n","image_paths"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:21.807286Z","iopub.status.busy":"2023-09-08T13:24:21.806600Z","iopub.status.idle":"2023-09-08T13:24:21.820565Z","shell.execute_reply":"2023-09-08T13:24:21.819346Z","shell.execute_reply.started":"2023-09-08T13:24:21.807244Z"},"trusted":true},"outputs":[],"source":["\n","# since we have imbalanced classes we will create a custom sampler which will sample with given weights\n","class_dist, class_weights, bincount = calculate_class_weights_from_directory(test_dir)\n","test_sampler = torch.utils.data.WeightedRandomSampler(class_weights, len(class_weights), replacement=True)\n","print('Test')\n","print('given weights:' ,np.unique(class_weights))\n","print('counts: ', Counter(class_weights))\n","print('Test bincount: ', bincount)\n","# Create a DataLoader to load the data in batches\n","\n","balanced_test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, sampler = test_sampler, num_workers=2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:21.823189Z","iopub.status.busy":"2023-09-08T13:24:21.822285Z","iopub.status.idle":"2023-09-08T13:24:22.456613Z","shell.execute_reply":"2023-09-08T13:24:22.455429Z","shell.execute_reply.started":"2023-09-08T13:24:21.823154Z"},"id":"9ZV11k_qqQKI","trusted":true},"outputs":[],"source":["# Iterate through the data loader and print the shape of each batch\n","a = 1\n","for i, j in val_dataloader:\n","    print(i.shape)\n","    print(j.shape)\n","    a += 1\n","    if a == 3:\n","        break\n"]},{"cell_type":"markdown","metadata":{"id":"mtzXxiKqT7pA"},"source":["## Class and idx transform"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:22.459191Z","iopub.status.busy":"2023-09-08T13:24:22.458815Z","iopub.status.idle":"2023-09-08T13:24:22.469580Z","shell.execute_reply":"2023-09-08T13:24:22.468441Z","shell.execute_reply.started":"2023-09-08T13:24:22.459140Z"},"id":"wn34MI9mxIDx","outputId":"fdb73c3d-38fd-4045-f79c-7fb300fcbfc3","trusted":true},"outputs":[],"source":["train_dataset.classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:22.471863Z","iopub.status.busy":"2023-09-08T13:24:22.471415Z","iopub.status.idle":"2023-09-08T13:24:22.479066Z","shell.execute_reply":"2023-09-08T13:24:22.478146Z","shell.execute_reply.started":"2023-09-08T13:24:22.471829Z"},"id":"9uW7Q04MyQXA","trusted":true},"outputs":[],"source":["idx2class = {i:name for i, name in enumerate(train_dataset.classes)}\n","class2idx = {name:i for i, name in enumerate(train_dataset.classes)}"]},{"cell_type":"markdown","metadata":{"id":"_c0chr7brV7g"},"source":["# Vision Transformer(VIT)"]},{"cell_type":"markdown","metadata":{"id":"R6iGJxmL06tF"},"source":["![image](https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:22.481945Z","iopub.status.busy":"2023-09-08T13:24:22.480206Z","iopub.status.idle":"2023-09-08T13:24:22.494323Z","shell.execute_reply":"2023-09-08T13:24:22.493357Z","shell.execute_reply.started":"2023-09-08T13:24:22.481910Z"},"id":"K9ROB49WYFdX","trusted":true},"outputs":[],"source":["\n","class MLP(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_dim, config.mlp_dim)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","        self.out = nn.Linear(config.mlp_dim, config.hidden_dim)\n","\n","    def forward(self, x):\n","        x = F.gelu(self.dense(x))\n","        return self.out(self.dropout(x))\n","\n","class Encoder(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","\n","        self.attention = nn.MultiheadAttention(config.hidden_dim, config.num_heads)\n","        self.mlp = MLP(config)\n","        self.norm1 = nn.LayerNorm(config.hidden_dim)\n","        self.norm2 = nn.LayerNorm(config.hidden_dim)\n","        self.attention_weights = None\n","\n","    def forward(self, x):\n","        n_x = self.norm1(x)\n","        attn_output, self.attention_weights = self.attention(n_x, n_x, n_x)\n","        x = x + attn_output\n","        return x + self.mlp(self.norm2(x))\n","\n","class EncoderStack(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.layers = nn.ModuleList([Encoder(config) for _ in range(config.num_layers)])\n","\n","    def forward(self, x):\n","        attention_weights = []\n","        for layer in self.layers:\n","            x = layer(x)\n","            attention_weights.append(layer.attention_weights)\n","        return x, attention_weights\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:22.496446Z","iopub.status.busy":"2023-09-08T13:24:22.496066Z","iopub.status.idle":"2023-09-08T13:24:22.518514Z","shell.execute_reply":"2023-09-08T13:24:22.517503Z","shell.execute_reply.started":"2023-09-08T13:24:22.496414Z"},"trusted":true},"outputs":[],"source":["\n","class VisionTransformer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","\n","        self.positional_embedding_layer = nn.Embedding(config.num_patches, config.hidden_dim).to(config.device)\n","        self.positional_embedding_layer.weight.data.uniform_(0, 1)\n","\n","        self.encoder = EncoderStack(config)  # Create multiple Encoder layers\n","        self.dense = nn.Linear(config.hidden_dim, config.hidden_dim)\n","        self.final_layer = nn.Linear(config.hidden_dim, 1)\n","        self.device = config.device\n","        self.patch_linear_proj = nn.Linear(config.patching_elements, config.hidden_dim, bias=False).to(config.device)\n","        self.patcher = Patcher((config.patch_size,config.patch_size))\n","        # Initialize the class token as a learnable parameter\n","#         self.class_token = nn.Parameter(torch.zeros(1, 1, config.hidden_dim))  # Learnable class token\n","        self.positional_embeddings = self.build_positional_embedding(config.num_patches, config.hidden_dim)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def create_patches(self, x):\n","        batch = x.shape[0]\n","        patches = self.patcher(x).to(self.device)\n","#         print('patcher: ', patches.shape,patches.device)\n","        patches = self.patch_linear_proj(patches)\n","#         print('proj: ', patches.shape,patches.device)\n","        # Add a learnable class token to the patch embeddings\n","#         class_token = self.class_token.expand(batch, -1, -1)\n","#         patches = torch.cat((class_token.to(self.device), patches), dim=1)\n","\n","        return patches\n","\n","    def build_positional_embedding(self, num_patches, embed_dim):\n","        positions = torch.arange(0, num_patches).view(1, -1).to(self.device)  # Move to the correct device\n","#         print('posss: ',positions.device)\n","        positional_embeddings = self.positional_embedding_layer(positions)\n","        return positional_embeddings\n","\n","\n","    def test(self, x):\n","        batch_size = x.shape[0]\n","        with torch.no_grad():  # Use torch.no_grad() for inference\n","            print('input: ', x.shape, x.device)\n","            a = self.create_patches(x)\n","            print('patches: ', a.shape, a.device)\n","            a = a + self.positional_embeddings.repeat(batch_size, 1, 1)\n","            print('pos emb: ', a.shape, a.device)\n","            a, b = self.encoder(a)\n","            print('encoder: ', a.shape, a.device)\n","            a = self.dense(a)\n","            print('dense: ', a.shape, a.device)\n","            a = self.final_layer(a)\n","            print('final: ', a.shape, a.device)\n","            a,_ = a.max(dim=1)\n","            print('max: ', a.shape, a.device)\n","            print('attention weights: ', len(b), '*', b[0].shape)\n","            return a, b\n","\n","    def forward(self, x):\n","        batch_size = x.shape[0]\n","        patch_embeddings = self.create_patches(x)\n","        positional_embeddings = self.positional_embeddings.repeat(batch_size, 1, 1)  # Repeat for each batch\n","        patch_embeddings = patch_embeddings + positional_embeddings\n","        encoded_output, attention_weights_list = self.encoder(patch_embeddings)\n","        # Calculate the mean over the 'num_patches' dimension\n","        encoded_output = encoded_output.mean(dim=1)\n","        # Apply the final linear layer and sigmoid\n","        encoded_output = self.final_layer(self.dense(encoded_output))\n","        encoded_output = self.sigmoid(encoded_output)\n","\n","        # Reshape to (batch, 1) shape\n","        encoded_output = encoded_output.view(batch_size, 1)\n","\n","        return encoded_output, attention_weights_list\n","\n","\n","class Patcher(nn.Module):\n","    def __init__(self, patch_size):\n","        super(Patcher, self).__init__()\n","        self.patch_size = patch_size\n","\n","    def forward(self, images):\n","        if images.dim() == 3:\n","            images = images.unsqueeze(0)  # Convert a single image to a batch\n","\n","        batch_size, channels, height, width = images.size()\n","        patch_height, patch_width = self.patch_size\n","\n","        # Calculate the number of patches in the height and width dimensions\n","        num_patches_height = height // patch_height\n","        num_patches_width = width // patch_width\n","        num_patches = num_patches_height * num_patches_width\n","\n","        patches = images.unfold(2, patch_height, patch_height).unfold(3, patch_width, patch_width)\n","        patches = patches.contiguous().view(batch_size, channels, -1, patch_height, patch_width)\n","        patches = patches.permute(0, 2, 3, 4, 1).contiguous().view(batch_size, num_patches, -1)\n","\n","        return patches"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:22.520135Z","iopub.status.busy":"2023-09-08T13:24:22.519601Z","iopub.status.idle":"2023-09-08T13:24:26.730026Z","shell.execute_reply":"2023-09-08T13:24:26.729007Z","shell.execute_reply.started":"2023-09-08T13:24:22.520102Z"},"id":"1GFC6nAx5Rr5","outputId":"09dd08ee-4c2d-44e9-a389-db3767382ef3","trusted":true},"outputs":[],"source":["vit = VisionTransformer(config)\n","vit = vit.to(device)\n","a = torch.rand((1,3,512,512))\n","output, attention_weights = vit.test(a.to(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:26.731925Z","iopub.status.busy":"2023-09-08T13:24:26.731566Z","iopub.status.idle":"2023-09-08T13:24:26.771494Z","shell.execute_reply":"2023-09-08T13:24:26.770523Z","shell.execute_reply.started":"2023-09-08T13:24:26.731891Z"},"trusted":true},"outputs":[],"source":["output, attention_weights = vit(a)\n","print(output.shape, len(attention_weights))\n","print('outputs: ',output)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:26.773316Z","iopub.status.busy":"2023-09-08T13:24:26.772955Z","iopub.status.idle":"2023-09-08T13:24:26.780869Z","shell.execute_reply":"2023-09-08T13:24:26.779852Z","shell.execute_reply.started":"2023-09-08T13:24:26.773282Z"},"id":"o21a7F-OsDi1","outputId":"2d821527-10cf-437d-8f9a-90ed68ea8045","trusted":true},"outputs":[],"source":["criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(vit.parameters(), lr = 5e-4)\n","\n","config.device = device\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:24:26.788892Z","iopub.status.busy":"2023-09-08T13:24:26.788598Z","iopub.status.idle":"2023-09-08T13:24:26.815062Z","shell.execute_reply":"2023-09-08T13:24:26.814203Z","shell.execute_reply.started":"2023-09-08T13:24:26.788869Z"},"id":"ccmf6O_Dzjpo","outputId":"ee2b5eea-0a90-48a9-c1b3-72d28d9b63db","trusted":true},"outputs":[],"source":["try:\n","    vit.load_state_dict(torch.load(model_path))\n","    print('weights loaded!')\n","except:\n","    print('No saved model')\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:30:28.554737Z","iopub.status.busy":"2023-09-08T13:30:28.554357Z","iopub.status.idle":"2023-09-08T13:30:28.569864Z","shell.execute_reply":"2023-09-08T13:30:28.568815Z","shell.execute_reply.started":"2023-09-08T13:30:28.554701Z"},"id":"dWfodsp6sDgZ","trusted":true},"outputs":[],"source":["def train_model(\n","    model,\n","    dataloader,\n","    optimizer,\n","    loss_function,\n","    num_epochs=10,\n","    device=\"cpu\",\n","    data_percent=1.0,\n","    steps_per_epoch=None,\n","    save_on_every_n_epochs=5,\n","    model_path=None,\n","):\n","    model.to(device)\n","    print(f\"{model.__class__.__name__} Running on: {device}\")\n","\n","    data_size = int(data_percent * len(dataloader)) if steps_per_epoch is None else steps_per_epoch\n","\n","    # if steps_per_epoch is None:\n","    #     steps_per_epoch = len(dataloader) // num_epochs\n","\n","    for epoch in range(num_epochs):\n","        total_loss = 0.0\n","        total_correct_predictions = 0\n","        total_samples = 0\n","\n","        epoch_progress = tqdm(\n","            dataloader, desc=f\"Epoch [{epoch + 1:2}/{num_epochs:2}]\"\n","        )\n","\n","        last_update_time = time.time() - 1.0  # Initialize to ensure the first update\n","\n","        for j, batch in enumerate(epoch_progress):\n","            image, label = batch\n","            image = image.to(device)\n","            label = label.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs, _ = model(image)\n","            outputs = outputs.squeeze()\n","            predictions = torch.round(outputs)\n","#             print('pred: ', predictions.shape)\n","#             print('label: ',label.shape, label.dtype)\n","# #             print('outsq: ', outputs.shape, outputs.dtype)\n","#             print('label: ',label)\n","#             print('outsq: ', predictions, predictions.dtype)\n","\n","            loss = loss_function(outputs, label.to(outputs.dtype))\n","            loss.backward(retain_graph=True)\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            total_correct_predictions += (predictions.to(torch.int32) == label.to(torch.int32)).sum().item()\n","            total_samples += label.size()[0]\n","\n","            formatted_loss = f\"{loss.item():.8f}\"\n","            accuracy = (total_correct_predictions / total_samples) * 100\n","            formatted_accuracy = f\"{accuracy:.2f}%\"\n","            \n","            \n","            current_time = time.time()\n","            if current_time - last_update_time > epoch_progress.mininterval:\n","                epoch_progress.set_postfix(\n","                    {\"Loss\": formatted_loss, \"Accuracy\": formatted_accuracy}\n","                )\n","                last_update_time = current_time\n","\n","            if steps_per_epoch is not None and j + 1 >= steps_per_epoch:\n","                break\n","        \n","        average_loss = total_loss / data_size\n","        average_accuracy = (total_correct_predictions / (total_samples + 1e-7)) * 100\n","\n","        print(\n","            f\"\\nEpoch [{epoch + 1:2}/{num_epochs:2}] - Average Loss: {average_loss:.8f} - Average Accuracy: {average_accuracy:.2f}%\"\n","        )\n","        print()\n","\n","        if (epoch+1) % save_on_every_n_epochs == 0 and model_path is not None:\n","            torch.save(model.state_dict(), model_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:30:29.786375Z","iopub.status.busy":"2023-09-08T13:30:29.786012Z","iopub.status.idle":"2023-09-08T13:34:09.913431Z","shell.execute_reply":"2023-09-08T13:34:09.912339Z","shell.execute_reply.started":"2023-09-08T13:30:29.786343Z"},"id":"b_6j7ChKsDb1","outputId":"afce2e82-4c98-4389-d80e-4c335ab51e71","trusted":true},"outputs":[],"source":["train_model(vit, train_dataloader, optimizer, criterion, num_epochs=40, device = device, save_on_every_n_epochs=4, model_path=model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:25:03.509629Z","iopub.status.idle":"2023-09-08T13:25:03.510774Z","shell.execute_reply":"2023-09-08T13:25:03.510526Z","shell.execute_reply.started":"2023-09-08T13:25:03.510499Z"},"id":"qg1c3icEsDVY","outputId":"a595652d-15a5-49d4-ecc2-1658ce23e761","trusted":true},"outputs":[],"source":["torch.save(vit.state_dict(), model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:25:03.512259Z","iopub.status.idle":"2023-09-08T13:25:03.512739Z","shell.execute_reply":"2023-09-08T13:25:03.512504Z","shell.execute_reply.started":"2023-09-08T13:25:03.512483Z"},"id":"b_fCJt5hsDSV","trusted":true},"outputs":[],"source":["!ls -la models -h\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:37:57.967435Z","iopub.status.busy":"2023-09-08T13:37:57.966318Z","iopub.status.idle":"2023-09-08T13:37:57.978669Z","shell.execute_reply":"2023-09-08T13:37:57.977592Z","shell.execute_reply.started":"2023-09-08T13:37:57.967383Z"},"id":"koc1zlKxsDHP","trusted":true},"outputs":[],"source":["def validate_model(\n","    model,\n","    dataloader,\n","    loss_function,\n","    device=\"cpu\",\n","):\n","    model.to(device)\n","    print(f\"Validating {model.__class__.__name__} on: {device}\")\n","\n","    total_loss = 0.0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    with torch.inference_mode():\n","        validation_progress = tqdm(\n","            dataloader, desc=\"Validation\"\n","        )\n","\n","        for batch in validation_progress:\n","            image, label = batch\n","            image = image.to(device)\n","            label = label.to(device)\n","\n","            outputs, _ = model(image)\n","            outputs = outputs.squeeze()\n","            predictions = torch.round(outputs)\n","\n","            loss = loss_function(outputs, label.to(outputs.dtype))\n","            \n","            total_loss += loss.item()\n","            correct_predictions += (predictions.to(torch.int32) == label.to(torch.int32)).sum().item()\n","            total_samples += label.size()[0]\n","\n","            formatted_loss = f\"{loss.item():.8f}\"\n","            accuracy = (correct_predictions / total_samples) * 100\n","            formatted_accuracy = f\"{accuracy:.2f}%\"\n","\n","            validation_progress.set_postfix(\n","                {\"Loss\": formatted_loss, \"Accuracy\": formatted_accuracy}\n","            )\n","\n","    average_loss = total_loss / len(dataloader)\n","    accuracy = (correct_predictions / (total_samples + 1e-7)) * 100\n","\n","    print(f\"Validation - Average Loss: {average_loss:.8f} - Accuracy: {accuracy:.2f}%\")\n","    print()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:37:58.594295Z","iopub.status.busy":"2023-09-08T13:37:58.593915Z","iopub.status.idle":"2023-09-08T13:37:59.093957Z","shell.execute_reply":"2023-09-08T13:37:59.092749Z","shell.execute_reply.started":"2023-09-08T13:37:58.594263Z"},"id":"6M_Of77IsDEl","outputId":"098d9a4f-3a2a-4a91-f9d1-a66118b3505e","trusted":true},"outputs":[],"source":["validate_model(vit, val_dataloader, criterion, device = config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:37:59.096620Z","iopub.status.busy":"2023-09-08T13:37:59.096228Z","iopub.status.idle":"2023-09-08T13:37:59.107878Z","shell.execute_reply":"2023-09-08T13:37:59.106627Z","shell.execute_reply.started":"2023-09-08T13:37:59.096581Z"},"id":"k-9mmZ2tsDAj","trusted":true},"outputs":[],"source":["def evaluate_model(model, dataloader, device=\"cpu\"):\n","    model.to(device)\n","\n","    y_true = []\n","    y_pred = []\n","\n","    with torch.inference_mode():  # Disable gradient computation during evaluation\n","        for batch in tqdm(dataloader):\n","            images, labels = batch\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs, _ = model(images)\n","            outputs = outputs.squeeze()\n","            predictions = torch.round(outputs)\n","\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predictions.cpu().numpy())\n","\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='binary')\n","    recall = recall_score(y_true, y_pred, average='binary')\n","    f1 = f1_score(y_true, y_pred, average='binary')\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","\n","    return y_true, y_pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:37:59.110056Z","iopub.status.busy":"2023-09-08T13:37:59.109630Z","iopub.status.idle":"2023-09-08T13:37:59.121533Z","shell.execute_reply":"2023-09-08T13:37:59.120514Z","shell.execute_reply.started":"2023-09-08T13:37:59.110015Z"},"trusted":true},"outputs":[],"source":["def make_cm(ytrue, ypred, title=None):\n","    # Create a confusion matrix\n","    cm = confusion_matrix(ytrue, ypred)\n","\n","    # Create a figure with two subplots\n","    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","    # Plot the confusion matrix in the first subplot\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[0])\n","    axes[0].set_xlabel(\"Predicted Labels\")\n","    axes[0].set_ylabel(\"True Labels\")\n","    axes[0].set_title(\"Confusion Matrix\" if title is None else title)\n","\n","    # Calculate and display performance metrics in the second subplot\n","    accuracy = accuracy_score(ytrue, ypred)\n","    precision = precision_score(ytrue, ypred, average='binary')\n","    recall = recall_score(ytrue, ypred, average='binary')\n","    f1 = f1_score(ytrue, ypred, average='binary')\n","\n","    # Set the x and y coordinates for each text label\n","    x_pos = 0.2  # Adjust these coordinates as needed\n","    y_pos = 0.6\n","\n","    # Print the text on the plot without x-label and y-label\n","    axes[1].text(x_pos, y_pos, f\"Accuracy: {accuracy:.4f}\", fontsize=12)\n","    axes[1].text(x_pos, y_pos - 0.1, f\"Precision: {precision:.4f}\", fontsize=12)\n","    axes[1].text(x_pos, y_pos - 0.2, f\"Recall: {recall:.4f}\", fontsize=12)\n","    axes[1].text(x_pos, y_pos - 0.3, f\"F1 Score: {f1:.4f}\", fontsize=12)\n","\n","    # Remove x-label and y-label from the second subplot\n","    axes[1].axes.get_xaxis().set_visible(False)\n","    axes[1].axes.get_yaxis().set_visible(False)\n","\n","    # Adjust the layout of subplots\n","    plt.tight_layout()\n","\n","    # Show the plot\n","    plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["##  Testing on Given(unbalanced) test_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:37:59.347950Z","iopub.status.busy":"2023-09-08T13:37:59.347598Z","iopub.status.idle":"2023-09-08T13:38:13.731017Z","shell.execute_reply":"2023-09-08T13:38:13.728086Z","shell.execute_reply.started":"2023-09-08T13:37:59.347921Z"},"id":"iEuF-WcesC97","outputId":"ae6f360b-4dc1-4b8f-b6d1-f6fb8c6d2085","trusted":true},"outputs":[],"source":["#testing on test_data\n","ytrue, ypred = evaluate_model(vit, test_dataloader, device = config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:38:13.735212Z","iopub.status.busy":"2023-09-08T13:38:13.734223Z","iopub.status.idle":"2023-09-08T13:38:13.744954Z","shell.execute_reply":"2023-09-08T13:38:13.743752Z","shell.execute_reply.started":"2023-09-08T13:38:13.735172Z"},"id":"0lA3dsv1J5uX","outputId":"3fefa3e0-4fc5-444c-caa8-d95667b427bd","trusted":true},"outputs":[],"source":["np.bincount(ytrue), np.bincount(ypred)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:38:13.746439Z","iopub.status.busy":"2023-09-08T13:38:13.746137Z","iopub.status.idle":"2023-09-08T13:38:14.121615Z","shell.execute_reply":"2023-09-08T13:38:14.120570Z","shell.execute_reply.started":"2023-09-08T13:38:13.746404Z"},"trusted":true},"outputs":[],"source":["make_cm(ytrue, ypred,title='test data unbalanced')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:38:14.124742Z","iopub.status.busy":"2023-09-08T13:38:14.124361Z","iopub.status.idle":"2023-09-08T13:38:14.128820Z","shell.execute_reply":"2023-09-08T13:38:14.127768Z","shell.execute_reply.started":"2023-09-08T13:38:14.124707Z"},"trusted":true},"outputs":[],"source":["# Explaination"]},{"cell_type":"markdown","metadata":{},"source":["*  **Accuracy**  \n","*  **Precision**    : (higher the number less likely to predict false positive.) which means model is likely to not classify a healthy person as pneumonia patient \n","*  **Recall**       : (higher the number less likely to predict false negetive.) which means model is likely to not classify a pneumonia patient as healthy person.\n","* **F1**          : (higher the better) this is sweet spot between precision and recall."]},{"cell_type":"markdown","metadata":{},"source":["We should aim for higher Recall. because it is better to classify a healthy person as sick compared to classifing a sick person as healthy. \n"]},{"cell_type":"markdown","metadata":{},"source":["## Testing on balanced test_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:38:14.140932Z","iopub.status.busy":"2023-09-08T13:38:14.140496Z","iopub.status.idle":"2023-09-08T13:38:29.907782Z","shell.execute_reply":"2023-09-08T13:38:29.906635Z","shell.execute_reply.started":"2023-09-08T13:38:14.140899Z"},"trusted":true},"outputs":[],"source":["#testing on test_data\n","ytrue, ypred = evaluate_model(vit, balanced_test_dataloader, device = config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:38:29.910707Z","iopub.status.busy":"2023-09-08T13:38:29.909922Z","iopub.status.idle":"2023-09-08T13:38:29.919470Z","shell.execute_reply":"2023-09-08T13:38:29.918406Z","shell.execute_reply.started":"2023-09-08T13:38:29.910639Z"},"trusted":true},"outputs":[],"source":["np.bincount(ytrue), np.bincount(ypred)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-08T13:38:29.921505Z","iopub.status.busy":"2023-09-08T13:38:29.921145Z","iopub.status.idle":"2023-09-08T13:38:30.282553Z","shell.execute_reply":"2023-09-08T13:38:30.281620Z","shell.execute_reply.started":"2023-09-08T13:38:29.921470Z"},"trusted":true},"outputs":[],"source":["make_cm(ytrue, ypred,title='test data balanced')"]},{"cell_type":"markdown","metadata":{},"source":["# Model 's performance can be improved with more unbiased data. "]},{"cell_type":"markdown","metadata":{},"source":["In conclusion, the model's performance and capabilities are influenced by various factors, including its architecture, training data, and the problem it was designed to solve. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"denv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
